@inproceedings{abbas2024enhancing,
 author = {Abbas, Momin and Zhou, Yi and Ram, Parikshit and Baracaldo, Nathalie and Samulowitz, Horst and Salonidis, Theodoros and Chen, Tianyi},
 booktitle = {International Conference on Artificial Intelligence and Statistics},
 organization = {PMLR},
 pages = {307--315},
 title = {Enhancing In-context Learning via Linear Probe Calibration},
 url = {http://arxiv.org/abs/2401.12406v1},
 year = {2024}
}

@article{bahri2024explaining,
 author = {Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
 journal = {Proceedings of the National Academy of Sciences},
 number = {27},
 pages = {e2311878121},
 publisher = {National Academy of Sciences},
 title = {Explaining Neural Scaling Laws},
 url = {http://dx.doi.org/10.1073/pnas.2311878121},
 volume = {121},
 year = {2024}
}

@inproceedings{basile-etal-2019-semeval,
 abstract = {The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.},
 address = {Minneapolis, Minnesota, USA},
 author = {Basile, Valerio  and
Bosco, Cristina  and
Fersini, Elisabetta  and
Nozza, Debora  and
Patti, Viviana  and
Rangel Pardo, Francisco Manuel  and
Rosso, Paolo  and
Sanguinetti, Manuela},
 booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
 doi = {10.18653/v1/S19-2007},
 editor = {May, Jonathan  and
Shutova, Ekaterina  and
Herbelot, Aurelie  and
Zhu, Xiaodan  and
Apidianaki, Marianna  and
Mohammad, Saif M.},
 month = {June},
 pages = {54--63},
 publisher = {Association for Computational Linguistics},
 title = {{s}em{e}val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in {t}witter},
 url = {https://aclanthology.org/S19-2007/},
 year = {2019}
}

@inproceedings{biderman2023pythia,
 author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and Oâ€™Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {2397--2430},
 title = {Pythia: a Suite for Analyzing Large Language Models Across Training and Scaling},
 url = {http://arxiv.org/pdf/2304.01373},
 year = {2023}
}

@inproceedings{bruneticl,
 author = {Brunet, Marc-Etienne and Anderson, Ashton and Zemel, Richard},
 booktitle = {R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
 title = {Icl-markup: Structuring In-context Learning Using Soft-token Tags},
 url = {https://www.semanticscholar.org/paper/1eddc4e36632342309885131a63ebd0bac1d837f},
 year = {2023}
}

@inproceedings{chen2023relation,
 author = {Chen, Yanda and Zhao, Chen and Yu, Zhou and McKeown, Kathleen and He, He},
 booktitle = {2023 Findings of the Association for Computational Linguistics: EMNLP 2023},
 organization = {Association for Computational Linguistics (ACL)},
 pages = {155--167},
 title = {On the Relation Between Sensitivity and Accuracy in In-context Learning},
 url = {http://arxiv.org/abs/2209.07661v3},
 year = {2023}
}

@inproceedings{cho2024revisiting,
 author = {Cho, Hakaze and Kato, Mariko and Sakai, Yoshihiro and Inoue, Naoya},
 booktitle = {The Thirteenth International Conference on Learning Representations},
 title = {Revisiting In-context Learning Inference Circuit in Large Language Models},
 url = {https://arxiv.org/abs/2410.04468},
 year = {2025}
}

@article{cho2024token,
 author = {Cho, Hakaze and Sakai, Yoshihiro and Kato, Mariko and Tanaka, Kenshiro and Ishii, Akira and Inoue, Naoya},
 journal = {arXiv preprint arXiv:2406.16535},
 title = {Token-based Decision Criteria Are Suboptimal in In-context Learning},
 url = {http://arxiv.org/abs/2406.16535v2},
 year = {2024}
}

@article{collins2024context,
 author = {Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
 journal = {arXiv preprint arXiv:2402.11639},
 title = {In-context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness},
 url = {https://arxiv.org/abs/2402.11639},
 year = {2024}
}

@inproceedings{commitmentbank,
 author = {De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
 booktitle = {proceedings of Sinn und Bedeutung},
 number = {2},
 pages = {107--124},
 title = {The Commitmentbank: Investigating Projection in Naturally Occurring Discourse},
 url = {https://www.semanticscholar.org/paper/39e801ca0dbc69c3697f118e24dac964abb63d4a},
 volume = {23},
 year = {2019}
}

@inproceedings{dai2023can,
 author = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
 booktitle = {ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
 title = {Why Can Gpt Learn In-context? Language Models Implicitly Perform Gradient Descent as Meta-optimizers},
 url = {http://arxiv.org/abs/2212.10559v3},
 year = {2023}
}

@inproceedings{dbpedia,
 author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
 booktitle = {international semantic web conference},
 organization = {Springer},
 pages = {722--735},
 title = {Dbpedia: a Nucleus for a Web of Open Data},
 url = {https://link.springer.com/content/pdf/10.1007/978-3-540-76298-0_52.pdf},
 year = {2007}
}

@article{dong2022survey,
 author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
 journal = {arXiv preprint arXiv:2301.00234},
 title = {A Survey on In-context Learning},
 url = {https://arxiv.org/abs/2301.00234},
 year = {2022}
}

@inproceedings{ECE1,
 author = {Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
 booktitle = {Proceedings of the AAAI conference on artificial intelligence},
 number = {1},
 title = {Obtaining Well Calibrated Probabilities Using Bayesian Binning},
 url = {https://ojs.aaai.org/index.php/AAAI/article/download/9602/9461},
 volume = {29},
 year = {2015}
}

@article{falcon40b,
 author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
 title = {{falcon-40b}: an Open Large Language Model with State-of-the-art Performance},
 url = {https://huggingface.co/tiiuae/falcon-40b},
 year = {2023}
}

@inproceedings{fei2023mitigating,
 author = {Fei, Yu and Hou, Yifan and Chen, Zeming and Bosselut, Antoine},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {14014--14031},
 title = {Mitigating Label Biases for In-context Learning},
 url = {http://arxiv.org/abs/2305.19148v3},
 year = {2023}
}

@article{FP,
 author = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},
 journal = {Journal of the Association for Information Science and Technology},
 title = {Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts},
 url = {http://arxiv.org/abs/1307.5336v2},
 volume = {65},
 year = {2014}
}

@inproceedings{gibert2018hate,
 address = {Brussels, Belgium},
 author = {de Gibert, Ona  and
Perez, Naiara  and
Garc{\'\i}a-Pablos, Aitor  and
Cuadros, Montse},
 booktitle = {Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)},
 doi = {10.18653/v1/W18-5102},
 month = {October},
 pages = {11--20},
 publisher = {Association for Computational Linguistics},
 title = {{hate Speech Dataset from a White Supremacy Forum}},
 url = {https://www.aclweb.org/anthology/W18-5102},
 year = {2018}
}

@inproceedings{gonen2023demystifying,
 author = {Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A and Zettlemoyer, Luke},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 pages = {10136--10148},
 title = {Demystifying Prompts in Language Models via Perplexity Estimation},
 url = {http://arxiv.org/abs/2212.04037v2},
 year = {2023}
}

@software{gpt-neo,
 author = {Black, Sid and
Leo, Gao and
Wang, Phil and
Leahy, Connor and
Biderman, Stella},
 doi = {10.5281/zenodo.5297715},
 month = {March},
 note = {{If you use this software, please cite it using 
these metadata.}},
 publisher = {Zenodo},
 title = {{gpt-neo: Large Scale Autoregressive Language Modeling with Mesh-tensorflow}},
 url = {https://doi.org/10.5281/zenodo.5297715},
 version = {1.0},
 year = {2021}
}

@article{grattafiori2024llama,
 author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
 journal = {arXiv e-prints},
 pages = {arXiv--2407},
 title = {The Llama 3 Herd of Models},
 url = {http://arxiv.org/abs/2407.21783v3},
 year = {2024}
}

@inproceedings{gu2023pre,
 author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {4849--4870},
 title = {Pre-training to Learn in Context},
 url = {http://arxiv.org/pdf/2305.09137},
 year = {2023}
}

@inproceedings{guo2017calibration,
 author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {1321--1330},
 title = {On Calibration of Modern Neural Networks},
 url = {http://arxiv.org/abs/1706.04599v2},
 year = {2017}
}

@inproceedings{han2023prototypical,
 author = {Han, Zhixiong and Hao, Yaru and Dong, Li and Sun, Yutao and Wei, Furu},
 booktitle = {The Eleventh International Conference on Learning Representations},
 title = {Prototypical Calibration for Few-shot Learning of Language Models},
 url = {http://arxiv.org/abs/2205.10183v2},
 year = {2023}
}

@inproceedings{han2023understanding,
 author = {Han, Xiaochuang and Simig, Daniel and Mihaylov, Todor and Tsvetkov, Yulia and Celikyilmaz, Asli and Wang, Tianlu},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {12660--12673},
 title = {Understanding In-context Learning via Supportive Pretraining Data},
 url = {http://arxiv.org/abs/2306.15091v1},
 year = {2023}
}

@article{hao2022structured,
 author = {Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
 journal = {arXiv preprint arXiv:2212.06713},
 title = {Structured Prompting: Scaling In-context Learning to 1,000 Examples},
 url = {http://arxiv.org/abs/2212.06713v1},
 year = {2022}
}

@article{huang2023context,
 author = {Huang, Yu and Cheng, Yuan and Liang, Yingbin},
 journal = {arXiv preprint arXiv:2310.05249},
 title = {In-context Convergence of Transformers},
 url = {https://arxiv.org/abs/2310.05249},
 year = {2023}
}

@article{iyer2022opt,
 author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
 journal = {arXiv preprint arXiv:2212.12017},
 title = {Opt-iml: Scaling Language Model Instruction Meta Learning Through the Lens of Generalization},
 url = {https://arxiv.org/abs/2212.12017},
 year = {2022}
}

@article{jeon2024information,
 author = {Jeon, Hong Jun and Lee, Jason D and Lei, Qi and Van Roy, Benjamin},
 journal = {arXiv preprint arXiv:2401.15530},
 title = {An Information-theoretic Analysis of In-context Learning},
 url = {https://arxiv.org/abs/2401.15530},
 year = {2024}
}

@inproceedings{jiang2023generative,
 author = {Jiang, Zhongtao and Zhang, Yuanzhe and Liu, Cao and Zhao, Jun and Liu, Kang},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 pages = {2312--2333},
 title = {Generative Calibration for In-context Learning},
 url = {https://www.semanticscholar.org/paper/bd2bbaa226be8fe6564e878e262942dd54c4fdad},
 year = {2023}
}

@article{kaplan2020scaling,
 author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
 journal = {arXiv preprint arXiv:2001.08361},
 title = {Scaling Laws for Neural Language Models},
 url = {http://arxiv.org/abs/2001.08361v1},
 year = {2020}
}

@inproceedings{kossen2024context,
 author = {Kossen, Jannik and Gal, Yarin and Rainforth, Tom},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {In-context Learning Learns Label Relationships but Is Not Conventional Learning},
 url = {http://arxiv.org/abs/2307.12375v4},
 year = {2024}
}

@inproceedings{levesque2012winograd,
 author = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
 booktitle = {Thirteenth international conference on the principles of knowledge representation and reasoning},
 title = {The Winograd Schema Challenge},
 url = {https://www.semanticscholar.org/paper/128cb6b891aee1b5df099acb48e2efecfcff689f},
 year = {2012}
}

@inproceedings{li2023finding,
 author = {Li, Xiaonan and Qiu, Xipeng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 pages = {6219--6235},
 title = {Finding Support Examples for In-context Learning},
 url = {http://arxiv.org/abs/2302.13539v3},
 year = {2023}
}

@inproceedings{li2023unified,
 author = {Li, Xiaonan and Lv, Kai and Yan, Hang and Lin, Tianyang and Zhu, Wei and Ni, Yuan and XIE, GUOTONG and Wang, Xiaoling and Qiu, Xipeng},
 booktitle = {The 61st Annual Meeting Of The Association For Computational Linguistics},
 title = {Unified Demonstration Retriever for In-context Learning},
 url = {http://arxiv.org/abs/2305.04320v2},
 year = {2023}
}

@article{li2024language,
 author = {Li, Jiaoda and Hou, Yifan and Sachan, Mrinmaya and Cotterell, Ryan},
 journal = {arXiv preprint arXiv:2406.04216},
 title = {What Do Language Models Learn in Context? the Structured Task Hypothesis},
 url = {http://arxiv.org/abs/2406.04216v3},
 year = {2024}
}

@article{lin2024awq,
 author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {87--100},
 title = {Awq: Activation-aware Weight Quantization for On-device Llm Compression and Acceleration},
 url = {https://www.semanticscholar.org/paper/db9507cdd3e2d7d9c90ed185bd831e55c62dcec9},
 volume = {6},
 year = {2024}
}

@inproceedings{liu2022makes,
 author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, William B and Carin, Lawrence and Chen, Weizhu},
 booktitle = {Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
 pages = {100--114},
 title = {What Makes Good In-context Examples for Gpt-3?},
 url = {https://aclanthology.org/2022.deelio-1.10.pdf},
 year = {2022}
}

@article{liu2024let,
 author = {Liu, Yinpeng and Liu, Jiawei and Shi, Xiang and Cheng, Qikai and Lu, Wei},
 journal = {arXiv preprint arXiv:2402.10738},
 title = {Let's Learn Step by Step: Enhancing In-context Learning Ability with Curriculum Learning},
 url = {http://arxiv.org/abs/2402.10738v2},
 year = {2024}
}

@inproceedings{lu2022fantastically,
 author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {8086--8098},
 title = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-shot Prompt Order Sensitivity},
 url = {http://arxiv.org/abs/2104.08786v2},
 year = {2022}
}

@article{mavromatis2023examples,
 author = {Mavromatis, Costas and Srinivasan, Balasubramaniam and Shen, Zhengyuan and Zhang, Jiani and Rangwala, Huzefa and Faloutsos, Christos and Karypis, George},
 journal = {arXiv preprint arXiv:2310.20046},
 title = {Which Examples to Annotate for In-context Learning? Towards Effective and Efficient Selection},
 url = {http://arxiv.org/abs/2310.20046v1},
 year = {2023}
}

@inproceedings{min2022metaicl,
 author = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {2791--2809},
 title = {Metaicl: Learning to Learn in Context},
 url = {https://aclanthology.org/2022.naacl-main.201.pdf},
 year = {2022}
}

@inproceedings{min2022noisy,
 author = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {5316--5330},
 title = {Noisy Channel Language Model Prompting for Few-shot Text Classification},
 url = {http://arxiv.org/abs/2108.04106v3},
 year = {2022}
}

@inproceedings{min2022rethinking,
 author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {11048--11064},
 title = {Rethinking the Role of Demonstrations: What Makes In-context Learning Work?},
 url = {https://aclanthology.org/2022.emnlp-main.759.pdf},
 year = {2022}
}

@inproceedings{mohammad-etal-2018-semeval,
 abstract = {We present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring the affectual state of a person from their tweet. For each task, we created labeled data from English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. emotion intensity ordinal classification, 3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification. Seventy-five teams (about 200 team members) participated in the shared task. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze systems for consistent bias towards a particular race or gender. The data is made freely available to further improve our understanding of how people convey emotions through language.},
 address = {New Orleans, Louisiana},
 author = {Mohammad, Saif  and
Bravo-Marquez, Felipe  and
Salameh, Mohammad  and
Kiritchenko, Svetlana},
 booktitle = {Proceedings of the 12th International Workshop on Semantic Evaluation},
 doi = {10.18653/v1/S18-1001},
 editor = {Apidianaki, Marianna  and
Mohammad, Saif M.  and
May, Jonathan  and
Shutova, Ekaterina  and
Bethard, Steven  and
Carpuat, Marine},
 month = {June},
 pages = {1--17},
 publisher = {Association for Computational Linguistics},
 title = {{s}em{e}val-2018 Task 1: Affect in Tweets},
 url = {https://aclanthology.org/S18-1001/},
 year = {2018}
}

@inproceedings{MR,
 author = {Pang, Bo and Lee, Lillian},
 booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACLâ€™05)},
 pages = {115--124},
 title = {Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales},
 url = {http://arxiv.org/abs/cs/0506075v1},
 year = {2005}
}

@inproceedings{mrpc,
 author = {Dolan, Bill and Brockett, Chris},
 booktitle = {Third international workshop on paraphrasing (IWP2005)},
 title = {Automatically Constructing a Corpus of Sentential Paraphrases},
 url = {https://www.semanticscholar.org/paper/475354f10798f110d34792b6d88f31d6d5cb099e},
 year = {2005}
}

@inproceedings{naeini2015obtaining,
 author = {Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
 booktitle = {Proceedings of the AAAI conference on artificial intelligence},
 number = {1},
 title = {Obtaining Well Calibrated Probabilities Using Bayesian Binning},
 url = {https://ojs.aaai.org/index.php/AAAI/article/download/9602/9461},
 volume = {29},
 year = {2015}
}

@article{olsson2022context,
 author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
 journal = {arXiv preprint arXiv:2209.11895},
 title = {In-context Learning and Induction Heads},
 url = {https://arxiv.org/abs/2209.11895},
 year = {2022}
}

@inproceedings{pan2023context,
 author = {Pan, Jane and Gao, Tianyu and Chen, Howard and Chen, Danqi},
 booktitle = {The 61st Annual Meeting Of The Association For Computational Linguistics},
 title = {What In-context Learning" Learns" In-context: Disentangling Task Recognition and Task Learning},
 url = {http://arxiv.org/pdf/2305.09731},
 year = {2023}
}

@article{pillutla2021mauve,
 author = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
 journal = {Advances in Neural Information Processing Systems},
 pages = {4816--4828},
 title = {Mauve: Measuring the Gap Between Neural Text and Human Text Using Divergence Frontiers},
 url = {https://www.semanticscholar.org/paper/8484fdb56e4690927dc0191ede11c2d24bc5e2ef},
 volume = {34},
 year = {2021}
}

@article{qin2023context,
 author = {Qin, Chengwei and Zhang, Aston and Dagar, Anirudh and Ye, Wenming},
 journal = {arXiv preprint arXiv:2310.09881},
 title = {In-context Learning with Iterative Demonstration Selection},
 url = {http://arxiv.org/abs/2310.09881v4},
 year = {2023}
}

@article{qwen,
 author = {Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
 journal = {arXiv preprint arXiv:2309.16609},
 title = {Qwen Technical Report},
 url = {http://arxiv.org/abs/2309.16609v1},
 year = {2023}
}

@article{qwen2,
 title = {Qwen2 Technical Report},
 url = {http://arxiv.org/abs/2407.10671v4},
 year = {2024}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
 journal = {OpenAI blog},
 number = {8},
 pages = {9},
 title = {Language Models Are Unsupervised Multitask Learners},
 url = {https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe},
 volume = {1},
 year = {2019}
}

@inproceedings{RTE1,
 author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
 booktitle = {Machine learning challenges workshop},
 organization = {Springer},
 pages = {177--190},
 title = {The Pascal Recognising Textual Entailment Challenge},
 url = {https://www.semanticscholar.org/paper/e808f28d411a958c5db81ceb111beb2638698f47},
 year = {2005}
}

@inproceedings{RTE2,
 author = {Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
 booktitle = {Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment},
 pages = {785--794},
 title = {The Second Pascal Recognising Textual Entailment Challenge},
 url = {https://www.semanticscholar.org/paper/136326377c122560768db674e35f5bcd6de3bc40},
 volume = {7},
 year = {2006}
}

@inproceedings{RTE3,
 author = {Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
 booktitle = {Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
 pages = {1--9},
 title = {The Third Pascal Recognizing Textual Entailment Challenge},
 url = {http://dl.acm.org/ft_gateway.cfm?id=1654538&type=pdf},
 year = {2007}
}

@article{RTE5,
 author = {Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
 journal = {TAC},
 pages = {8},
 publisher = {Citeseer},
 title = {The Fifth Pascal Recognizing Textual Entailment Challenge.},
 url = {https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf},
 volume = {7},
 year = {2009}
}

@article{schaeffer2024emergent,
 author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
 journal = {Advances in Neural Information Processing Systems},
 title = {Are Emergent Abilities of Large Language Models a Mirage?},
 url = {http://arxiv.org/abs/2304.15004v2},
 volume = {36},
 year = {2024}
}

@inproceedings{shi2024incontext,
 author = {Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and Margaret Li and Xi Victoria Lin and Noah A. Smith and Luke Zettlemoyer and Wen-tau Yih and Mike Lewis},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {In-context Pretraining: Language Modeling Beyond Document Boundaries},
 url = {https://openreview.net/forum?id=LXVswInHOo},
 year = {2024}
}

@article{shi2024larger,
 author = {Shi, Zhenmei and Wei, Junyi and Xu, Zhuoyan and Liang, Yingyu},
 journal = {arXiv preprint arXiv:2405.19592},
 title = {Why Larger Language Models Do In-context Learning Differently?},
 url = {http://arxiv.org/abs/2405.19592v1},
 year = {2024}
}

@article{srivastava2023beyond,
 author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri and others},
 journal = {Transactions on machine learning research},
 title = {Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models},
 url = {http://arxiv.org/abs/2206.04615v3},
 year = {2023}
}

@inproceedings{sst2,
 address = {Seattle, Washington, USA},
 author = {Socher, Richard  and
Perelygin, Alex  and
Wu, Jean  and
Chuang, Jason  and
Manning, Christopher D.  and
Ng, Andrew  and
Potts, Christopher},
 booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
 month = {October},
 pages = {1631--1642},
 publisher = {Association for Computational Linguistics},
 title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
 url = {https://www.aclweb.org/anthology/D13-1170},
 year = {2013}
}

@inproceedings{su2023selective,
 author = {Hongjin SU and Jungo Kasai and Chen Henry Wu and Weijia Shi and Tianlu Wang and Jiayi Xin and Rui Zhang and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {Selective Annotation Makes Language Models Better Few-shot Learners},
 url = {https://openreview.net/forum?id=qY1hlv7gwg},
 year = {2023}
}

@inproceedings{subj,
 abstract = {Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.},
 address = {USA},
 author = {Wang, Sida and Manning, Christopher D.},
 booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2},
 location = {Jeju Island, Korea},
 numpages = {5},
 pages = {90â€“94},
 publisher = {Association for Computational Linguistics},
 series = {ACL '12},
 title = {Baselines and Bigrams: Simple, Good Sentiment and Topic Classification},
 url = {https://www.semanticscholar.org/paper/5e9fa46f231c59e6573f9a116f77f53703347659},
 year = {2012}
}

@inproceedings{suzgun2023challenging,
 author = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
 pages = {13003--13051},
 title = {Challenging Big-bench Tasks and Whether Chain-of-thought Can Solve Them},
 url = {http://arxiv.org/abs/2210.09261v1},
 year = {2023}
}

@book{taguchi1987taguchi,
 author = {Taguchi, Genichi and Konishi, Seiso},
 publisher = {ASI press},
 title = {Taguchi Methods: Orthogonal Arrays and Linear Graphs; Tools for Quality Engineering},
 year = {1987}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
 journal = {arXiv preprint arXiv:2307.09288},
 title = {Llama 2: Open Foundation and Fine-tuned Chat Models},
 url = {http://arxiv.org/abs/2307.09288v2},
 year = {2023}
}

@inproceedings{trec1,
 author = {Li, Xin  and
Roth, Dan},
 booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
 title = {Learning Question Classifiers},
 url = {https://www.aclweb.org/anthology/C02-1150},
 year = {2002}
}

@inproceedings{trec2,
 author = {Hovy, Eduard  and
Gerber, Laurie  and
Hermjakob, Ulf  and
Lin, Chin-Yew  and
Ravichandran, Deepak},
 booktitle = {Proceedings of the First International Conference on Human Language Technology Research},
 title = {Toward Semantics-based Answer Pinpointing},
 url = {https://www.aclweb.org/anthology/H01-1069},
 year = {2001}
}

@article{van2024context,
 author = {Van, Minh-Hao and Wu, Xintao and others},
 journal = {arXiv preprint arXiv:2402.11750},
 title = {In-context Learning Demonstration Selection via Influence Analysis},
 url = {http://arxiv.org/abs/2402.11750v2},
 year = {2024}
}

@article{voronov2024mind,
 author = {Voronov, Anton and Wolf, Lena and Ryabinin, Max},
 journal = {arXiv preprint arXiv:2401.06766},
 title = {Mind Your Format: Towards Consistent Evaluation of In-context Learning Improvements},
 url = {http://arxiv.org/abs/2401.06766v3},
 year = {2024}
}

@inproceedings{wang2019glue,
 author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
 booktitle = {7th International Conference on Learning Representations, ICLR 2019},
 title = {Glue: a Multi-task Benchmark and Analysis Platform for Natural Language Understanding},
 url = {https://www.aclweb.org/anthology/W18-5446.pdf},
 year = {2019}
}

@misc{wang2021gpt,
 author = {Wang, Ben and Komatsuzaki, Aran},
 title = {Gpt-j-6b: a 6 Billion Parameter Autoregressive Language Model},
 url = {https://huggingface.co/EleutherAI/gpt-j-6b},
 year = {2021}
}

@inproceedings{wang2023label,
 author = {Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 pages = {9840--9855},
 title = {Label Words Are Anchors: an Information Flow Perspective for Understanding In-context Learning},
 url = {http://arxiv.org/abs/2305.14160v4},
 year = {2023}
}

@inproceedings{wei2022finetuned,
 author = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
 booktitle = {International Conference on Learning Representations},
 title = {Finetuned Language Models Are Zero-shot Learners},
 url = {https://openreview.net/forum?id=gEZrGCozdqR},
 year = {2022}
}

@article{wei2023larger,
 author = {Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
 journal = {arXiv preprint arXiv:2303.03846},
 title = {Larger Language Models Do In-context Learning Differently},
 url = {http://arxiv.org/abs/2303.03846v2},
 year = {2023}
}

@inproceedings{wei2023symbol,
 author = {Wei, Jerry and Hou, Le and Lampinen, Andrew and Chen, Xiangning and Huang, Da and Tay, Yi and Chen, Xinyun and Lu, Yifeng and Zhou, Denny and Ma, Tengyu and others},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 pages = {968--979},
 title = {Symbol Tuning Improves In-context Learning in Language Models},
 url = {http://arxiv.org/abs/2305.08298v2},
 year = {2023}
}

@article{wies2024learnability,
 author = {Wies, Noam and Levine, Yoav and Shashua, Amnon},
 journal = {Advances in Neural Information Processing Systems},
 title = {The Learnability of In-context Learning},
 url = {http://arxiv.org/abs/2303.07895v1},
 volume = {36},
 year = {2024}
}

@inproceedings{wu2023self,
 author = {Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {1423--1436},
 title = {Self-adaptive In-context Learning: an Information Compression Perspective for In-context Example Selection and Ordering},
 url = {http://arxiv.org/abs/2212.10375v2},
 year = {2023}
}

@inproceedings{xu2023knn,
 author = {Benfeng Xu and Quan Wang and Zhendong Mao and Yajuan Lyu and Qiaoqiao She and Yongdong Zhang},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {\$k\${nn} Prompting: Beyond-context Learning with Calibration-free Nearest Neighbor Inference},
 url = {https://openreview.net/forum?id=fe2S7736sNS},
 year = {2023}
}

@inproceedings{yoo2022ground,
 author = {Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-goo and Kim, Taeuk},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {2422--2437},
 title = {Ground-truth Labels Matter: a Deeper Look into Input-label Demonstrations},
 url = {http://arxiv.org/abs/2205.12685v2},
 year = {2022}
}

@inproceedings{Zhang2015CharacterlevelCN,
 author = {Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
 booktitle = {NIPS},
 title = {Character-level Convolutional Networks for Text Classification},
 url = {https://www.semanticscholar.org/paper/51a55df1f023571a7e07e338ee45a3e3d66ef73e},
 year = {2015}
}

@article{zhang2022opt,
 author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
 journal = {arXiv preprint arXiv:2205.01068},
 title = {Opt: Open Pre-trained Transformer Language Models},
 url = {https://arxiv.org/abs/2205.01068},
 year = {2022}
}

@inproceedings{zhao2021calibrate,
 author = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {12697--12706},
 title = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
 url = {http://arxiv.org/abs/2102.09690v2},
 year = {2021}
}

@article{zhao2024noisyicl,
 author = {Zhao, Yufeng and Sakai, Yoshihiro and Inoue, Naoya},
 journal = {arXiv preprint arXiv:2402.05515},
 title = {Noisyicl: a Little Noise in Model Parameters Calibrates In-context Learning},
 url = {https://arxiv.org/abs/2402.05515},
 year = {2024}
}

@inproceedings{zhou2024batch,
 author = {Han Zhou and Xingchen Wan and Lev Proleev and Diana Mincu and Jilin Chen and Katherine A Heller and Subhrajit Roy},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {Batch Calibration: Rethinking Calibration for In-context Learning and Prompt Engineering},
 url = {https://openreview.net/forum?id=L3FHMoKZcS},
 year = {2024}
}
